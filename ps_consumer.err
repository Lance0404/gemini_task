(gemini_task) root@ubuntu:/home/lance/gemini_task# python ps_consumer.py
19/04/10 17:14:05 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.111 instead (on interface ens160)
19/04/10 17:14:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/04/10 17:14:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/04/10 17:14:07 INFO VerifiableProperties: Verifying properties
19/04/10 17:14:07 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/10 17:14:07 INFO VerifiableProperties: Property group.id is overridden to
19/04/10 17:14:07 INFO VerifiableProperties: Property zookeeper.connect is overridden to
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/10 17:14:07 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/10 17:14:07 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/10 17:14:07 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/10 17:14:07 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
Traceback (most recent call last):
  File "ps_consumer.py", line 70, in <module>
    directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], kafka_param)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/streaming/kafka.py", line 146, in createDirectStream
    ssc._jssc, kafkaParams, set(topics), jfromOffsets)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o26.createDirectStreamWithoutMessageHandler.
: org.apache.spark.SparkException: java.nio.channels.ClosedChannelException
org.apache.spark.SparkException: Couldn't find leader offsets for Set([firewall,0])
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:387)
    at scala.util.Either.fold(Either.scala:98)
    at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:386)
    at org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:223)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStream(KafkaUtils.scala:721)
    at org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper.createDirectStreamWithoutMessageHandler(KafkaUtils.scala:689)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)

19/04/10 17:14:07 DEBUG PythonGatewayServer: Exiting due to broken pipe from Python driver
19/04/10 17:14:07 INFO SparkContext: Invoking stop() from shutdown hook
(gemini_task) root@ubuntu:/home/lance/gemini_task# 19/04/10 17:14:07 INFO SparkUI: Stopped Spark web UI at http://192.168.18.111:4040
19/04/10 17:14:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/10 17:14:07 INFO MemoryStore: MemoryStore cleared
19/04/10 17:14:07 INFO BlockManager: BlockManager stopped
19/04/10 17:14:07 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/10 17:14:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/10 17:14:07 INFO SparkContext: Successfully stopped SparkContext
19/04/10 17:14:07 INFO ShutdownHookManager: Shutdown hook called
19/04/10 17:14:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3b81c34-e4a7-4cdf-8d6e-85847d7a2efa/pyspark-b468370c-c426-45a1-b180-d4d70fe97a5b
19/04/10 17:14:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-71d81019-2c9f-47a1-869c-2c1bc849b67b
19/04/10 17:14:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3b81c34-e4a7-4cdf-8d6e-85847d7a2efa
