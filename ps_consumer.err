19/04/11 11:21:51 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.18.111 instead (on interface ens160)
19/04/11 11:21:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/04/11 11:21:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/04/11 11:21:52 INFO SparkContext: Running Spark version 2.4.1
19/04/11 11:21:52 INFO SparkContext: Submitted application: ps_consumer
19/04/11 11:21:52 INFO SparkContext: Spark configuration:
spark.app.name=ps_consumer
spark.jars=file:///root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar
spark.logConf=True
spark.master=local[*]
spark.rdd.compress=True
spark.repl.local.jars=file:///root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar
spark.serializer.objectStreamReset=100
spark.submit.deployMode=client
19/04/11 11:21:52 INFO SecurityManager: Changing view acls to: root
19/04/11 11:21:52 INFO SecurityManager: Changing modify acls to: root
19/04/11 11:21:52 INFO SecurityManager: Changing view acls groups to: 
19/04/11 11:21:52 INFO SecurityManager: Changing modify acls groups to: 
19/04/11 11:21:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/04/11 11:21:52 INFO Utils: Successfully started service 'sparkDriver' on port 44692.
19/04/11 11:21:52 INFO SparkEnv: Registering MapOutputTracker
19/04/11 11:21:52 INFO SparkEnv: Registering BlockManagerMaster
19/04/11 11:21:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/11 11:21:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/11 11:21:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-30079100-403e-4f52-a7cf-f66afc9719a1
19/04/11 11:21:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/04/11 11:21:52 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/11 11:21:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/11 11:21:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.18.111:4040
19/04/11 11:21:52 INFO SparkContext: Added JAR file:///root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar at spark://192.168.18.111:44692/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar with timestamp 1554952912869
19/04/11 11:21:52 INFO Executor: Starting executor ID driver on host localhost
19/04/11 11:21:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40565.
19/04/11 11:21:52 INFO NettyBlockTransferService: Server created on 192.168.18.111:40565
19/04/11 11:21:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/11 11:21:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.18.111, 40565, None)
19/04/11 11:21:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.111:40565 with 366.3 MB RAM, BlockManagerId(driver, 192.168.18.111, 40565, None)
19/04/11 11:21:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.18.111, 40565, None)
19/04/11 11:21:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.18.111, 40565, None)
19/04/11 11:21:53 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:53 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:53 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:53 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:53 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:53 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:53 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:53 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:53 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$13) +++
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:53 DEBUG ClosureCleaner:      public static final long org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$13.serialVersionUID
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$13.apply(java.lang.Object)
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final scala.Tuple2 org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$13.apply(kafka.message.MessageAndMetadata)
19/04/11 11:21:53 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:53 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:53 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.streaming.kafka.KafkaUtilsPythonHelper$$anonfun$13) is now cleaned +++
<pyspark.streaming.kafka.KafkaDStream object at 0x7ff59fab9cf8>
19/04/11 11:21:53 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) +++
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public static final long org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.serialVersionUID
19/04/11 11:21:53 DEBUG ClosureCleaner:      private final org.apache.spark.streaming.api.python.TransformFunction org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.func$1
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final void org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(org.apache.spark.rdd.RDD,org.apache.spark.streaming.Time)
19/04/11 11:21:53 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:53 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:53 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) is now cleaned +++
19/04/11 11:21:53 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) +++
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public static final long org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.serialVersionUID
19/04/11 11:21:53 DEBUG ClosureCleaner:      private final org.apache.spark.streaming.api.python.TransformFunction org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.func$1
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final void org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(org.apache.spark.rdd.RDD,org.apache.spark.streaming.Time)
19/04/11 11:21:53 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:53 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:53 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) is now cleaned +++
19/04/11 11:21:53 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) +++
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public static final long org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.serialVersionUID
19/04/11 11:21:53 DEBUG ClosureCleaner:      private final org.apache.spark.streaming.api.python.TransformFunction org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.func$1
19/04/11 11:21:53 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:53 DEBUG ClosureCleaner:      public final void org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(org.apache.spark.rdd.RDD,org.apache.spark.streaming.Time)
19/04/11 11:21:53 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:53 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:53 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:53 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1) is now cleaned +++
19/04/11 11:21:53 DEBUG JobScheduler: Starting JobScheduler
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@414b65a9
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@1dd845e6
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@37bfa821
19/04/11 11:21:53 INFO ForEachDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO ForEachDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO ForEachDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1d920406
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@414b65a9
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@1dd845e6
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@37bfa821
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@76e5c30
19/04/11 11:21:53 INFO ForEachDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO ForEachDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO ForEachDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@58c6de4e
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@414b65a9
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@1dd845e6
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@37bfa821
19/04/11 11:21:53 INFO PythonTransformedDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO PythonTransformedDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO PythonTransformedDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@76e5c30
19/04/11 11:21:53 INFO ForEachDStream: Slide time = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Storage level = Serialized 1x Replicated
19/04/11 11:21:53 INFO ForEachDStream: Checkpoint interval = null
19/04/11 11:21:53 INFO ForEachDStream: Remember interval = 5000 ms
19/04/11 11:21:53 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@12728bd7
19/04/11 11:21:53 INFO RecurringTimer: Started timer for JobGenerator at time 1554952915000
19/04/11 11:21:53 INFO JobGenerator: Started JobGenerator at 1554952915000 ms
19/04/11 11:21:53 INFO JobScheduler: Started JobScheduler
19/04/11 11:21:53 DEBUG StreamingContext: Adding shutdown hook
19/04/11 11:21:53 INFO StreamingContext: StreamingContext started
19/04/11 11:21:55 DEBUG RecurringTimer: Callback for JobGenerator called at time 1554952915000
19/04/11 11:21:55 DEBUG JobGenerator: Got event GenerateJobs(1554952915000 ms)
19/04/11 11:21:55 DEBUG DStreamGraph: Generating jobs for time 1554952915000 ms
19/04/11 11:21:55 DEBUG PythonTransformedDStream: Time 1554952915000 ms is valid
19/04/11 11:21:55 DEBUG PythonTransformedDStream: Time 1554952915000 ms is valid
19/04/11 11:21:55 DEBUG DirectKafkaInputDStream: Time 1554952915000 ms is valid
19/04/11 11:21:55 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:55 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:55 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:55 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:55 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from localhost:9092
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:55 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:55 DEBUG PythonTransformedDStream: Time 1554952915000 ms is valid
19/04/11 11:21:55 DEBUG DStreamGraph: Generated 3 jobs for time 1554952915000 ms
19/04/11 11:21:55 INFO JobScheduler: Added jobs for time 1554952915000 ms
19/04/11 11:21:55 DEBUG JobGenerator: Got event DoCheckpoint(1554952915000 ms,false)
19/04/11 11:21:55 INFO JobScheduler: Starting job streaming job 1554952915000 ms.0 from job set of time 1554952915000 ms
19/04/11 11:21:55 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) +++
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:55 DEBUG ClosureCleaner:      public static final long org.apache.spark.api.python.PythonRDD$$anonfun$3.serialVersionUID
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(java.lang.Object)
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final byte[][] org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(scala.collection.Iterator)
19/04/11 11:21:55 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:55 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:55 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) is now cleaned +++
19/04/11 11:21:55 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
19/04/11 11:21:55 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
19/04/11 11:21:55 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:55 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:55 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
19/04/11 11:21:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153
19/04/11 11:21:55 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions
19/04/11 11:21:55 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:153)
19/04/11 11:21:55 INFO DAGScheduler: Parents of final stage: List()
19/04/11 11:21:55 INFO DAGScheduler: Missing parents: List()
19/04/11 11:21:55 DEBUG DAGScheduler: submitStage(ResultStage 0)
19/04/11 11:21:55 DEBUG DAGScheduler: missing: List()
19/04/11 11:21:55 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:53), which has no missing parents
19/04/11 11:21:55 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)
19/04/11 11:21:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.5 KB, free 366.3 MB)
19/04/11 11:21:55 DEBUG BlockManager: Put block broadcast_0 locally took  17 ms
19/04/11 11:21:55 DEBUG BlockManager: Putting block broadcast_0 without replication took  18 ms
19/04/11 11:21:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)
19/04/11 11:21:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.18.111:40565 (size: 4.4 KB, free: 366.3 MB)
19/04/11 11:21:55 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
19/04/11 11:21:55 DEBUG BlockManager: Told master about block broadcast_0_piece0
19/04/11 11:21:55 DEBUG BlockManager: Put block broadcast_0_piece0 locally took  4 ms
19/04/11 11:21:55 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  4 ms
19/04/11 11:21:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
19/04/11 11:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[4] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
19/04/11 11:21:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/04/11 11:21:55 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
19/04/11 11:21:55 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: ANY
19/04/11 11:21:55 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
19/04/11 11:21:55 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: ANY
19/04/11 11:21:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7782 bytes)
19/04/11 11:21:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/04/11 11:21:55 INFO Executor: Fetching spark://192.168.18.111:44692/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar with timestamp 1554952912869
19/04/11 11:21:55 DEBUG TransportClientFactory: Creating new connection to /192.168.18.111:44692
19/04/11 11:21:55 DEBUG AbstractByteBuf: -Dio.netty.buffer.bytebuf.checkAccessible: true
19/04/11 11:21:55 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@42e0f9d3
19/04/11 11:21:55 DEBUG TransportClientFactory: Connection to /192.168.18.111:44692 successful, running bootstraps...
19/04/11 11:21:55 INFO TransportClientFactory: Successfully created connection to /192.168.18.111:44692 after 35 ms (0 ms spent in bootstraps)
19/04/11 11:21:55 DEBUG TransportClient: Sending stream request for /jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar to /192.168.18.111:44692
19/04/11 11:21:55 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 32768
19/04/11 11:21:55 DEBUG Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
19/04/11 11:21:55 DEBUG Recycler: -Dio.netty.recycler.linkCapacity: 16
19/04/11 11:21:55 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
19/04/11 11:21:55 INFO Utils: Fetching spark://192.168.18.111:44692/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar to /tmp/spark-5d34fe06-2832-45fe-a2fa-d8346d95630e/userFiles-e7f559f4-bdee-4de5-8e25-a2ae633c1746/fetchFileTemp6252883930815688508.tmp
19/04/11 11:21:55 INFO Executor: Adding file:/tmp/spark-5d34fe06-2832-45fe-a2fa-d8346d95630e/userFiles-e7f559f4-bdee-4de5-8e25-a2ae633c1746/spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar to class loader
19/04/11 11:21:55 DEBUG BlockManager: Getting local block broadcast_0
19/04/11 11:21:55 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
19/04/11 11:21:55 INFO KafkaRDD: Computing topic firewall, partition 0 offsets 0 -> 4000
19/04/11 11:21:55 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:55 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:55 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:55 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:55 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:55 INFO PythonRunner: Times: total = 65, boot = 6, init = 58, finish = 1
19/04/11 11:21:55 DEBUG PythonRunner: Exception thrown after task completion (likely due to cleanup)
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
19/04/11 11:21:55 DEBUG PythonRunner: Exception thrown after task completion (likely due to cleanup)
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
19/04/11 11:21:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3378 bytes result sent to driver
19/04/11 11:21:55 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
19/04/11 11:21:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 600 ms on localhost (executor driver) (1/1)
19/04/11 11:21:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/04/11 11:21:55 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58985
19/04/11 11:21:55 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:153) finished in 0.734 s
19/04/11 11:21:55 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
19/04/11 11:21:55 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 0.781856 s
-------------------------------------------
Time: 2019-04-11 11:21:55
-------------------------------------------
{'timestamp': '2019-04-01T14:50:00.000-0700', 'user': 'USER048', 'action': 'success', 'app': 'app_1', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.050-0700', 'user': 'USER064', 'action': 'success', 'app': 'app_2', 'server': 'SERVER02.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.300-0700', 'user': 'USER081', 'action': 'unknown', 'app': 'app_3', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.345-0700', 'user': 'USER045', 'action': 'success', 'app': 'app_3', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.377-0700', 'user': 'USER023', 'action': 'failed', 'app': 'app_4', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.383-0700', 'user': 'USER092', 'action': 'success', 'app': 'app_2', 'server': 'SERVER04.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.782-0700', 'user': 'USER091', 'action': 'success', 'app': 'app_2', 'server': 'SERVER01.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.825-0700', 'user': 'USER055', 'action': 'failed', 'app': 'app_3', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:00.974-0700', 'user': 'USER068', 'action': 'success', 'app': 'app_2', 'server': 'SERVER03.gemini.datalab'}
{'timestamp': '2019-04-01T14:50:01.031-0700', 'user': 'USER075', 'action': 'unknown', 'app': 'app_2', 'server': 'SERVER02.gemini.datalab'}
...

19/04/11 11:21:55 INFO JobScheduler: Finished job streaming job 1554952915000 ms.0 from job set of time 1554952915000 ms
19/04/11 11:21:55 INFO JobScheduler: Starting job streaming job 1554952915000 ms.1 from job set of time 1554952915000 ms
19/04/11 11:21:55 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) +++
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:55 DEBUG ClosureCleaner:      public static final long org.apache.spark.api.python.PythonRDD$$anonfun$3.serialVersionUID
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(java.lang.Object)
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final byte[][] org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(scala.collection.Iterator)
19/04/11 11:21:55 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:55 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:55 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) is now cleaned +++
19/04/11 11:21:55 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
19/04/11 11:21:55 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
19/04/11 11:21:55 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:55 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
19/04/11 11:21:55 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:55 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:55 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:55 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
19/04/11 11:21:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153
19/04/11 11:21:55 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:153) with 1 output partitions
19/04/11 11:21:55 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:153)
19/04/11 11:21:55 INFO DAGScheduler: Parents of final stage: List()
19/04/11 11:21:55 INFO DAGScheduler: Missing parents: List()
19/04/11 11:21:55 DEBUG DAGScheduler: submitStage(ResultStage 1)
19/04/11 11:21:55 DEBUG DAGScheduler: missing: List()
19/04/11 11:21:55 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53), which has no missing parents
19/04/11 11:21:55 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)
19/04/11 11:21:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.9 KB, free 366.3 MB)
19/04/11 11:21:55 DEBUG BlockManager: Put block broadcast_1 locally took  1 ms
19/04/11 11:21:55 DEBUG BlockManager: Putting block broadcast_1 without replication took  2 ms
19/04/11 11:21:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 366.3 MB)
19/04/11 11:21:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.18.111:40565 (size: 4.6 KB, free: 366.3 MB)
19/04/11 11:21:55 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
19/04/11 11:21:55 DEBUG BlockManager: Told master about block broadcast_1_piece0
19/04/11 11:21:55 DEBUG BlockManager: Put block broadcast_1_piece0 locally took  2 ms
19/04/11 11:21:55 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took  2 ms
19/04/11 11:21:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
19/04/11 11:21:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
19/04/11 11:21:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/04/11 11:21:55 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0
19/04/11 11:21:55 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: ANY
19/04/11 11:21:55 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
19/04/11 11:21:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7782 bytes)
19/04/11 11:21:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
19/04/11 11:21:55 DEBUG BlockManager: Getting local block broadcast_1
19/04/11 11:21:55 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
19/04/11 11:21:55 INFO KafkaRDD: Computing topic firewall, partition 0 offsets 0 -> 4000
19/04/11 11:21:55 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:55 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:55 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:55 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:55 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:55 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:56 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:56 INFO PythonRunner: Times: total = 56, boot = 6, init = 37, finish = 13
19/04/11 11:21:56 INFO PythonRunner: Times: total = 96, boot = 4, init = 36, finish = 56
19/04/11 11:21:56 INFO PythonRunner: Times: total = 132, boot = 4, init = 39, finish = 89
19/04/11 11:21:56 INFO PythonRunner: Times: total = 129, boot = 5, init = 124, finish = 0
19/04/11 11:21:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1627 bytes result sent to driver
19/04/11 11:21:56 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
19/04/11 11:21:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on localhost (executor driver) (1/1)
19/04/11 11:21:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/04/11 11:21:56 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:153) finished in 0.169 s
19/04/11 11:21:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
19/04/11 11:21:56 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:153, took 0.173960 s
-------------------------------------------
Time: 2019-04-11 11:21:55
-------------------------------------------
USER048
USER064
USER081
USER045
USER023
USER092
USER091
USER055
USER068
USER075
...

19/04/11 11:21:56 INFO JobScheduler: Finished job streaming job 1554952915000 ms.1 from job set of time 1554952915000 ms
19/04/11 11:21:56 INFO JobScheduler: Starting job streaming job 1554952915000 ms.2 from job set of time 1554952915000 ms
========= 2019-04-11 11:21:55 =========
19/04/11 11:21:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lance/gemini_task/spark-warehouse/').
19/04/11 11:21:56 INFO SharedState: Warehouse path is 'file:/home/lance/gemini_task/spark-warehouse/'.
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(36)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 36
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 36
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(17)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 17
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 17
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(1)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 1
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 1
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(20)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 20
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 20
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(43)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 43
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 43
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(21)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 21
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 21
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(48)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 48
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 48
19/04/11 11:21:56 DEBUG ContextCleaner: Got cleaning task CleanAccum(3)
19/04/11 11:21:56 DEBUG ContextCleaner: Cleaning accumulator 3
19/04/11 11:21:56 INFO ContextCleaner: Cleaned accumulator 3
19/04/11 11:21:56 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/11 11:21:56 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) +++
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:56 DEBUG ClosureCleaner:      public static final long org.apache.spark.api.python.PythonRDD$$anonfun$3.serialVersionUID
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(java.lang.Object)
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final byte[][] org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(scala.collection.Iterator)
19/04/11 11:21:56 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:56 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:56 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.api.python.PythonRDD$$anonfun$3) is now cleaned +++
19/04/11 11:21:56 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
19/04/11 11:21:56 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
19/04/11 11:21:56 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:56 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:56 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
19/04/11 11:21:56 INFO SparkContext: Starting job: runJob at PythonRDD.scala:153
19/04/11 11:21:56 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:153) with 1 output partitions
19/04/11 11:21:56 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:153)
19/04/11 11:21:56 INFO DAGScheduler: Parents of final stage: List()
19/04/11 11:21:56 INFO DAGScheduler: Missing parents: List()
19/04/11 11:21:56 DEBUG DAGScheduler: submitStage(ResultStage 2)
19/04/11 11:21:56 DEBUG DAGScheduler: missing: List()
19/04/11 11:21:56 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents
19/04/11 11:21:56 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)
19/04/11 11:21:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.1 KB, free 366.3 MB)
19/04/11 11:21:56 DEBUG BlockManager: Put block broadcast_2 locally took  1 ms
19/04/11 11:21:56 DEBUG BlockManager: Putting block broadcast_2 without replication took  1 ms
19/04/11 11:21:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.4 KB, free 366.3 MB)
19/04/11 11:21:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.18.111:40565 (size: 5.4 KB, free: 366.3 MB)
19/04/11 11:21:56 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
19/04/11 11:21:56 DEBUG BlockManager: Told master about block broadcast_2_piece0
19/04/11 11:21:56 DEBUG BlockManager: Put block broadcast_2_piece0 locally took  2 ms
19/04/11 11:21:56 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took  2 ms
19/04/11 11:21:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
19/04/11 11:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
19/04/11 11:21:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/04/11 11:21:56 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0
19/04/11 11:21:56 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: ANY
19/04/11 11:21:56 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
19/04/11 11:21:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7782 bytes)
19/04/11 11:21:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
19/04/11 11:21:56 DEBUG BlockManager: Getting local block broadcast_2
19/04/11 11:21:56 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
19/04/11 11:21:56 INFO KafkaRDD: Computing topic firewall, partition 0 offsets 0 -> 4000
19/04/11 11:21:56 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:56 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:56 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:56 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:56 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:56 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:56 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:56 INFO PythonRunner: Times: total = 24, boot = -635, init = 645, finish = 14
19/04/11 11:21:56 INFO PythonRunner: Times: total = 35, boot = -582, init = 597, finish = 20
19/04/11 11:21:56 INFO PythonRunner: Times: total = 73, boot = 6, init = 66, finish = 1
19/04/11 11:21:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1487 bytes result sent to driver
19/04/11 11:21:56 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
19/04/11 11:21:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 82 ms on localhost (executor driver) (1/1)
19/04/11 11:21:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/04/11 11:21:56 INFO PythonRunner: Times: total = 72, boot = -544, init = 566, finish = 50
19/04/11 11:21:56 DEBUG PythonRunner: Exception thrown after task completion (likely due to cleanup)
java.net.SocketException: Socket closed
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:348)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
19/04/11 11:21:56 INFO DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:153) finished in 0.099 s
19/04/11 11:21:56 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0
19/04/11 11:21:56 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:153, took 0.102240 s
19/04/11 11:21:56 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1) +++
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public static final long org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1.serialVersionUID
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final boolean org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1.batched$1
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1.apply(java.lang.Object)
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1.apply(scala.collection.Iterator)
19/04/11 11:21:56 DEBUG ClosureCleaner:  + inner classes: 1
19/04/11 11:21:56 DEBUG ClosureCleaner:      org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:56 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:56 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1) is now cleaned +++
19/04/11 11:21:56 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1) +++
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:56 DEBUG ClosureCleaner:      public static final long org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1.serialVersionUID
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared methods: 1
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1.apply(java.lang.Object)
19/04/11 11:21:56 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:56 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:56 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1) is now cleaned +++
19/04/11 11:21:56 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.SparkSession$$anonfun$6) +++
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.SparkSession$$anonfun$6.serialVersionUID
19/04/11 11:21:56 DEBUG ClosureCleaner:      private final org.apache.spark.sql.types.StructType org.apache.spark.sql.SparkSession$$anonfun$6.schema$1
19/04/11 11:21:56 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.SparkSession$$anonfun$6.apply(java.lang.Object)
19/04/11 11:21:56 DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.sql.SparkSession$$anonfun$6.apply(scala.collection.Iterator)
19/04/11 11:21:56 DEBUG ClosureCleaner:  + inner classes: 1
19/04/11 11:21:56 DEBUG ClosureCleaner:      org.apache.spark.sql.SparkSession$$anonfun$6$$anonfun$apply$5
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:56 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:56 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:56 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.SparkSession$$anonfun$6) is now cleaned +++
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(7)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 7
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 7
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(46)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 46
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 46
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(44)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 44
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 44
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(23)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 23
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 23
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(61)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 61
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 61
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(34)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 34
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 34
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(59)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 59
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 59
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(1)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning broadcast 1
19/04/11 11:21:57 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 1
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: removing broadcast 1
19/04/11 11:21:57 DEBUG BlockManager: Removing broadcast 1
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_1
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_1 of size 10088 dropped from memory (free 384058745)
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_1_piece0
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_1_piece0 of size 4671 dropped from memory (free 384063416)
19/04/11 11:21:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.18.111:40565 in memory (size: 4.6 KB, free: 366.3 MB)
19/04/11 11:21:57 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
19/04/11 11:21:57 DEBUG BlockManager: Told master about block broadcast_1_piece0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 1, response is 0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.18.111:44692
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaned broadcast 1
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(62)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 62
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 62
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(67)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 67
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 67
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(68)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 68
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 68
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(66)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 66
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 66
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(56)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 56
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 56
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(71)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 71
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 71
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(65)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 65
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 65
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(74)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 74
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 74
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(57)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 57
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 57
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(51)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 51
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 51
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(70)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 70
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 70
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(55)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 55
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 55
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(53)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 53
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 53
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(52)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 52
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 52
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(2)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning broadcast 2
19/04/11 11:21:57 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 2
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: removing broadcast 2
19/04/11 11:21:57 DEBUG BlockManager: Removing broadcast 2
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_2
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_2 of size 11328 dropped from memory (free 384074744)
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_2_piece0
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_2_piece0 of size 5481 dropped from memory (free 384080225)
19/04/11 11:21:57 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.18.111:40565 in memory (size: 5.4 KB, free: 366.3 MB)
19/04/11 11:21:57 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
19/04/11 11:21:57 DEBUG BlockManager: Told master about block broadcast_2_piece0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 2, response is 0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.18.111:44692
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaned broadcast 2
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(64)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 64
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 64
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(60)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 60
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 60
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(15)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 15
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 15
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(42)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 42
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 42
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(41)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 41
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 41
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(31)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 31
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 31
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(2)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 2
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 2
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(11)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 11
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 11
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning broadcast 0
19/04/11 11:21:57 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: removing broadcast 0
19/04/11 11:21:57 DEBUG BlockManager: Removing broadcast 0
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_0_piece0
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_0_piece0 of size 4491 dropped from memory (free 384084716)
19/04/11 11:21:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.18.111:40565 in memory (size: 4.4 KB, free: 366.3 MB)
19/04/11 11:21:57 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
19/04/11 11:21:57 DEBUG BlockManager: Told master about block broadcast_0_piece0
19/04/11 11:21:57 DEBUG BlockManager: Removing block broadcast_0
19/04/11 11:21:57 DEBUG MemoryStore: Block broadcast_0 of size 8672 dropped from memory (free 384093388)
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 0, response is 0
19/04/11 11:21:57 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.18.111:44692
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaned broadcast 0
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(19)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 19
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 19
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(8)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 8
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 8
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(12)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 12
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 12
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(4)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 4
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 4
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(16)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 16
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 16
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(39)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 39
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 39
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(28)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 28
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 28
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(14)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 14
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 14
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(40)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 40
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 40
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(63)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 63
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 63
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(50)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 50
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 50
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(24)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 24
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 24
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(25)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 25
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 25
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(5)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 5
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 5
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(54)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 54
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 54
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(35)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 35
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 35
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(29)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 29
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 29
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(49)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 49
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 49
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(9)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 9
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 9
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(6)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 6
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 6
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(47)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 47
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 47
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(45)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 45
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 45
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(18)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 18
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 18
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(37)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 37
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 37
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(30)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 30
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 30
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(38)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 38
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 38
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(58)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 58
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 58
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(32)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 32
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 32
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(69)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 69
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 69
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(26)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 26
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 26
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(10)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 10
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 10
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(75)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 75
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 75
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(72)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 72
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 72
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(13)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 13
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 13
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(27)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 27
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 27
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(22)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 22
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 22
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(33)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 33
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 33
19/04/11 11:21:57 DEBUG ContextCleaner: Got cleaning task CleanAccum(73)
19/04/11 11:21:57 DEBUG ContextCleaner: Cleaning accumulator 73
19/04/11 11:21:57 INFO ContextCleaner: Cleaned accumulator 73
19/04/11 11:21:57 DEBUG SparkSqlParser: Parsing command: firewall
19/04/11 11:21:58 DEBUG SparkSqlParser: Parsing command: select user, count(*) as ucount from firewall group by user
19/04/11 11:21:58 DEBUG Analyzer$ResolveReferences: Resolving 'user to user#0
19/04/11 11:21:58 DEBUG Analyzer$ResolveReferences: Resolving 'user to user#0
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$1: 
=== Result of Batch Resolution ===
!'Aggregate ['user], ['user, 'count(1) AS ucount#2]   Aggregate [user#0], [user#0, count(1) AS ucount#2L]
!+- 'UnresolvedRelation `firewall`                    +- SubqueryAlias `firewall`
!                                                        +- LogicalRDD [user#0], false
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$1: 
=== Result of Batch Cleanup ===
 Aggregate [user#0], [user#0, count(1) AS ucount#2L]   Aggregate [user#0], [user#0, count(1) AS ucount#2L]
 +- SubqueryAlias `firewall`                           +- SubqueryAlias `firewall`
    +- LogicalRDD [user#0], false                         +- LogicalRDD [user#0], false
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$1: 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(user#0 as string), None), unresolvedalias(cast(ucount#2L as string), None)]   Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]
 +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]                                                       +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]
    +- SubqueryAlias `firewall`                                                                                  +- SubqueryAlias `firewall`
       +- LogicalRDD [user#0], false                                                                                +- LogicalRDD [user#0], false
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$1: 
=== Result of Batch Cleanup ===
 Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]   Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]
 +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]                              +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]
    +- SubqueryAlias `firewall`                                                         +- SubqueryAlias `firewall`
       +- LogicalRDD [user#0], false                                                       +- LogicalRDD [user#0], false
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$2: 
=== Result of Batch Finish Analysis ===
 GlobalLimit 21                                                                            GlobalLimit 21
 +- LocalLimit 21                                                                          +- LocalLimit 21
    +- Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]      +- Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]
       +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]                                    +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]
!         +- SubqueryAlias `firewall`                                                               +- LogicalRDD [user#0], false
!            +- LogicalRDD [user#0], false                                                 
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$2: 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                            GlobalLimit 21
 +- LocalLimit 21                                                                          +- LocalLimit 21
!   +- Project [cast(user#0 as string) AS user#8, cast(ucount#2L as string) AS ucount#9]      +- Aggregate [user#0], [user#0, cast(count(1) as string) AS ucount#9]
!      +- Aggregate [user#0], [user#0, count(1) AS ucount#2L]                                    +- LogicalRDD [user#0], false
!         +- LogicalRDD [user#0], false                                                    
          
19/04/11 11:21:58 DEBUG BaseSessionStateBuilder$$anon$1: 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, StructField(user,StringType,true), StructField(ucount,StringType,false))), obj#14: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(user#8.toString, ucount#9.toString, StructField(user,StringType,true), StructField(ucount,StringType,false)), obj#14: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [user#8, ucount#9]                                                                                                                                                                                                                        +- LocalRelation <empty>, [user#8, ucount#9]
          
19/04/11 11:21:58 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, false].toString, StructField(user,StringType,true), StructField(ucount,StringType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_5);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[2];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */
/* 045 */       isNull_1 = false;
/* 046 */       if (!isNull_1) {
/* 047 */
/* 048 */         Object funcResult_0 = null;
/* 049 */         funcResult_0 = value_2.toString();
/* 050 */         value_1 = (java.lang.String) funcResult_0;
/* 051 */
/* 052 */       }
/* 053 */     }
/* 054 */     if (isNull_1) {
/* 055 */       values_0[0] = null;
/* 056 */     } else {
/* 057 */       values_0[0] = value_1;
/* 058 */     }
/* 059 */
/* 060 */     UTF8String value_4 = i.getUTF8String(1);
/* 061 */     boolean isNull_3 = true;
/* 062 */     java.lang.String value_3 = null;
/* 063 */     if (!false) {
/* 064 */
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_5 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_5);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[2];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */
/* 045 */       isNull_1 = false;
/* 046 */       if (!isNull_1) {
/* 047 */
/* 048 */         Object funcResult_0 = null;
/* 049 */         funcResult_0 = value_2.toString();
/* 050 */         value_1 = (java.lang.String) funcResult_0;
/* 051 */
/* 052 */       }
/* 053 */     }
/* 054 */     if (isNull_1) {
/* 055 */       values_0[0] = null;
/* 056 */     } else {
/* 057 */       values_0[0] = value_1;
/* 058 */     }
/* 059 */
/* 060 */     UTF8String value_4 = i.getUTF8String(1);
/* 061 */     boolean isNull_3 = true;
/* 062 */     java.lang.String value_3 = null;
/* 063 */     if (!false) {
/* 064 */
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 081 */
/* 082 */     return value_0;
/* 083 */   }
/* 084 */
/* 085 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 163.598226 ms
19/04/11 11:21:58 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 025 */     inputadapter_input_0 = inputs[0];
/* 026 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 027 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 028 */
/* 029 */   }
/* 030 */
/* 031 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 032 */   throws java.io.IOException {
/* 033 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 034 */
/* 035 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);
/* 036 */     UTF8String agg_value_7 = agg_isNull_7 ?
/* 037 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 038 */     long agg_value_8 = agg_bufferTerm_0.getLong(0);
/* 039 */
/* 040 */     boolean agg_isNull_11 = false;
/* 041 */     UTF8String agg_value_11 = null;
/* 042 */     if (!false) {
/* 043 */       agg_value_11 = UTF8String.fromString(String.valueOf(agg_value_8));
/* 044 */     }
/* 045 */     agg_mutableStateArray_0[1].reset();
/* 046 */
/* 047 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 048 */
/* 049 */     if (agg_isNull_7) {
/* 050 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 051 */     } else {
/* 052 */       agg_mutableStateArray_0[1].write(0, agg_value_7);
/* 053 */     }
/* 054 */
/* 055 */     agg_mutableStateArray_0[1].write(1, agg_value_11);
/* 056 */     append((agg_mutableStateArray_0[1].getRow()));
/* 057 */
/* 058 */   }
/* 059 */
/* 060 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0, long agg_expr_1_0) throws java.io.IOException {
/* 061 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 062 */
/* 063 */     // generate grouping key
/* 064 */     agg_mutableStateArray_0[0].reset();
/* 065 */
/* 066 */     agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 067 */
/* 068 */     if (agg_exprIsNull_0_0) {
/* 069 */       agg_mutableStateArray_0[0].setNullAt(0);
/* 070 */     } else {
/* 071 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 072 */     }
/* 073 */     int agg_value_2 = 48;
/* 074 */
/* 075 */     if (!agg_exprIsNull_0_0) {
/* 076 */       agg_value_2 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_2);
/* 077 */     }
/* 078 */     if (true) {
/* 079 */       // try to get the buffer from hash map
/* 080 */       agg_unsafeRowAggBuffer_0 =
/* 081 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_2);
/* 082 */     }
/* 083 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 084 */     // aggregation after processing all input rows.
/* 085 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 086 */       if (agg_sorter_0 == null) {
/* 087 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 088 */       } else {
/* 089 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 090 */       }
/* 091 */
/* 092 */       // the hash map had be spilled, it should have enough memory now,
/* 093 */       // try to allocate buffer again.
/* 094 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 095 */         (agg_mutableStateArray_0[0].getRow()), agg_value_2);
/* 096 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 097 */         // failed to allocate the first page
/* 098 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 099 */       }
/* 100 */     }
/* 101 */
/* 102 */     // common sub-expressions
/* 103 */
/* 104 */     // evaluate aggregate function
/* 105 */     long agg_value_5 = agg_unsafeRowAggBuffer_0.getLong(0);
/* 106 */
/* 107 */     long agg_value_4 = -1L;
/* 108 */     agg_value_4 = agg_value_5 + agg_expr_1_0;
/* 109 */     // update unsafe row buffer
/* 110 */     agg_unsafeRowAggBuffer_0.setLong(0, agg_value_4);
/* 111 */
/* 112 */   }
/* 113 */
/* 114 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 115 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 116 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 117 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 118 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 119 */       null : (inputadapter_row_0.getUTF8String(0));
/* 120 */       long inputadapter_value_1 = inputadapter_row_0.getLong(1);
/* 121 */
/* 122 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1);
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */
/* 126 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 127 */   }
/* 128 */
/* 129 */   protected void processNext() throws java.io.IOException {
/* 130 */     if (!agg_initAgg_0) {
/* 131 */       agg_initAgg_0 = true;
/* 132 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 133 */       agg_doAggregateWithKeys_0();
/* 134 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 135 */     }
/* 136 */
/* 137 */     // output the result
/* 138 */
/* 139 */     while (agg_mapIter_0.next()) {
/* 140 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 141 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 142 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 143 */
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */
/* 147 */     agg_mapIter_0.close();
/* 148 */     if (agg_sorter_0 == null) {
/* 149 */       agg_hashMap_0.free();
/* 150 */     }
/* 151 */   }
/* 152 */
/* 153 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 025 */     inputadapter_input_0 = inputs[0];
/* 026 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 027 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 028 */
/* 029 */   }
/* 030 */
/* 031 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 032 */   throws java.io.IOException {
/* 033 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 034 */
/* 035 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);
/* 036 */     UTF8String agg_value_7 = agg_isNull_7 ?
/* 037 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 038 */     long agg_value_8 = agg_bufferTerm_0.getLong(0);
/* 039 */
/* 040 */     boolean agg_isNull_11 = false;
/* 041 */     UTF8String agg_value_11 = null;
/* 042 */     if (!false) {
/* 043 */       agg_value_11 = UTF8String.fromString(String.valueOf(agg_value_8));
/* 044 */     }
/* 045 */     agg_mutableStateArray_0[1].reset();
/* 046 */
/* 047 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 048 */
/* 049 */     if (agg_isNull_7) {
/* 050 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 051 */     } else {
/* 052 */       agg_mutableStateArray_0[1].write(0, agg_value_7);
/* 053 */     }
/* 054 */
/* 055 */     agg_mutableStateArray_0[1].write(1, agg_value_11);
/* 056 */     append((agg_mutableStateArray_0[1].getRow()));
/* 057 */
/* 058 */   }
/* 059 */
/* 060 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0, long agg_expr_1_0) throws java.io.IOException {
/* 061 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 062 */
/* 063 */     // generate grouping key
/* 064 */     agg_mutableStateArray_0[0].reset();
/* 065 */
/* 066 */     agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 067 */
/* 068 */     if (agg_exprIsNull_0_0) {
/* 069 */       agg_mutableStateArray_0[0].setNullAt(0);
/* 070 */     } else {
/* 071 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 072 */     }
/* 073 */     int agg_value_2 = 48;
/* 074 */
/* 075 */     if (!agg_exprIsNull_0_0) {
/* 076 */       agg_value_2 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_2);
/* 077 */     }
/* 078 */     if (true) {
/* 079 */       // try to get the buffer from hash map
/* 080 */       agg_unsafeRowAggBuffer_0 =
/* 081 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_2);
/* 082 */     }
/* 083 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 084 */     // aggregation after processing all input rows.
/* 085 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 086 */       if (agg_sorter_0 == null) {
/* 087 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 088 */       } else {
/* 089 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 090 */       }
/* 091 */
/* 092 */       // the hash map had be spilled, it should have enough memory now,
/* 093 */       // try to allocate buffer again.
/* 094 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 095 */         (agg_mutableStateArray_0[0].getRow()), agg_value_2);
/* 096 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 097 */         // failed to allocate the first page
/* 098 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 099 */       }
/* 100 */     }
/* 101 */
/* 102 */     // common sub-expressions
/* 103 */
/* 104 */     // evaluate aggregate function
/* 105 */     long agg_value_5 = agg_unsafeRowAggBuffer_0.getLong(0);
/* 106 */
/* 107 */     long agg_value_4 = -1L;
/* 108 */     agg_value_4 = agg_value_5 + agg_expr_1_0;
/* 109 */     // update unsafe row buffer
/* 110 */     agg_unsafeRowAggBuffer_0.setLong(0, agg_value_4);
/* 111 */
/* 112 */   }
/* 113 */
/* 114 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 115 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 116 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 117 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 118 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 119 */       null : (inputadapter_row_0.getUTF8String(0));
/* 120 */       long inputadapter_value_1 = inputadapter_row_0.getLong(1);
/* 121 */
/* 122 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1);
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */
/* 126 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 127 */   }
/* 128 */
/* 129 */   protected void processNext() throws java.io.IOException {
/* 130 */     if (!agg_initAgg_0) {
/* 131 */       agg_initAgg_0 = true;
/* 132 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 133 */       agg_doAggregateWithKeys_0();
/* 134 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 135 */     }
/* 136 */
/* 137 */     // output the result
/* 138 */
/* 139 */     while (agg_mapIter_0.next()) {
/* 140 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 141 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 142 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 143 */
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */
/* 147 */     agg_mapIter_0.close();
/* 148 */     if (agg_sorter_0 == null) {
/* 149 */       agg_hashMap_0.free();
/* 150 */     }
/* 151 */   }
/* 152 */
/* 153 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 36.784999 ms
19/04/11 11:21:58 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private agg_FastHashMap_0 agg_fastHashMap_0;
/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter_0;
/* 014 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 017 */   private scala.collection.Iterator inputadapter_input_0;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 019 */
/* 020 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 021 */     this.references = references;
/* 022 */   }
/* 023 */
/* 024 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 025 */     partitionIndex = index;
/* 026 */     this.inputs = inputs;
/* 027 */
/* 028 */     agg_fastHashMap_0 = new agg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());
/* 029 */     agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 032 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 033 */
/* 034 */   }
/* 035 */
/* 036 */   public class agg_FastHashMap_0 {
/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;
/* 038 */     private int[] buckets;
/* 039 */     private int capacity = 1 << 16;
/* 040 */     private double loadFactor = 0.5;
/* 041 */     private int numBuckets = (int) (capacity / loadFactor);
/* 042 */     private int maxSteps = 2;
/* 043 */     private int numRows = 0;
/* 044 */     private Object emptyVBase;
/* 045 */     private long emptyVOff;
/* 046 */     private int emptyVLen;
/* 047 */     private boolean isBatchFull = false;
/* 048 */
/* 049 */     public agg_FastHashMap_0(
/* 050 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,
/* 051 */       InternalRow emptyAggregationBuffer) {
/* 052 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch
/* 053 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);
/* 054 */
/* 055 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));
/* 056 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();
/* 057 */
/* 058 */       emptyVBase = emptyBuffer;
/* 059 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;
/* 060 */       emptyVLen = emptyBuffer.length;
/* 061 */
/* 062 */       buckets = new int[numBuckets];
/* 063 */       java.util.Arrays.fill(buckets, -1);
/* 064 */     }
/* 065 */
/* 066 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key_0) {
/* 067 */       long h = hash(agg_key_0);
/* 068 */       int step = 0;
/* 069 */       int idx = (int) h & (numBuckets - 1);
/* 070 */       while (step < maxSteps) {
/* 071 */         // Return bucket index if it's either an empty slot or already contains the key
/* 072 */         if (buckets[idx] == -1) {
/* 073 */           if (numRows < capacity && !isBatchFull) {
/* 074 */             // creating the unsafe for new entry
/* 075 */             org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter
/* 076 */             = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(
/* 077 */               1, 32);
/* 078 */             agg_rowWriter.reset(); //TODO: investigate if reset or zeroout are actually needed
/* 079 */             agg_rowWriter.zeroOutNullBytes();
/* 080 */             agg_rowWriter.write(0, agg_key_0);
/* 081 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result
/* 082 */             = agg_rowWriter.getRow();
/* 083 */             Object kbase = agg_result.getBaseObject();
/* 084 */             long koff = agg_result.getBaseOffset();
/* 085 */             int klen = agg_result.getSizeInBytes();
/* 086 */
/* 087 */             UnsafeRow vRow
/* 088 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);
/* 089 */             if (vRow == null) {
/* 090 */               isBatchFull = true;
/* 091 */             } else {
/* 092 */               buckets[idx] = numRows++;
/* 093 */             }
/* 094 */             return vRow;
/* 095 */           } else {
/* 096 */             // No more space
/* 097 */             return null;
/* 098 */           }
/* 099 */         } else if (equals(idx, agg_key_0)) {
/* 100 */           return batch.getValueRow(buckets[idx]);
/* 101 */         }
/* 102 */         idx = (idx + 1) & (numBuckets - 1);
/* 103 */         step++;
/* 104 */       }
/* 105 */       // Didn't find it
/* 106 */       return null;
/* 107 */     }
/* 108 */
/* 109 */     private boolean equals(int idx, UTF8String agg_key_0) {
/* 110 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);
/* 111 */       return (row.getUTF8String(0).equals(agg_key_0));
/* 112 */     }
/* 113 */
/* 114 */     private long hash(UTF8String agg_key_0) {
/* 115 */       long agg_hash_0 = 0;
/* 116 */
/* 117 */       int agg_result_0 = 0;
/* 118 */       byte[] agg_bytes_0 = agg_key_0.getBytes();
/* 119 */       for (int i = 0; i < agg_bytes_0.length; i++) {
/* 120 */         int agg_hash_1 = agg_bytes_0[i];
/* 121 */         agg_result_0 = (agg_result_0 ^ (0x9e3779b9)) + agg_hash_1 + (agg_result_0 << 6) + (agg_result_0 >>> 2);
/* 122 */       }
/* 123 */
/* 124 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_0 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);
/* 125 */
/* 126 */       return agg_hash_0;
/* 127 */     }
/* 128 */
/* 129 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {
/* 130 */       return batch.rowIterator();
/* 131 */     }
/* 132 */
/* 133 */     public void close() {
/* 134 */       batch.close();
/* 135 */     }
/* 136 */
/* 137 */   }
/* 138 */
/* 139 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 140 */   throws java.io.IOException {
/* 141 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);
/* 142 */
/* 143 */     boolean agg_isNull_11 = agg_keyTerm_0.isNullAt(0);
/* 144 */     UTF8String agg_value_12 = agg_isNull_11 ?
/* 145 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 146 */     long agg_value_13 = agg_bufferTerm_0.getLong(0);
/* 147 */
/* 148 */     agg_mutableStateArray_0[1].reset();
/* 149 */
/* 150 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 151 */
/* 152 */     if (agg_isNull_11) {
/* 153 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 154 */     } else {
/* 155 */       agg_mutableStateArray_0[1].write(0, agg_value_12);
/* 156 */     }
/* 157 */
/* 158 */     agg_mutableStateArray_0[1].write(1, agg_value_13);
/* 159 */     append((agg_mutableStateArray_0[1].getRow()));
/* 160 */
/* 161 */   }
/* 162 */
/* 163 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {
/* 164 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 165 */     UnsafeRow agg_fastAggBuffer_0 = null;
/* 166 */
/* 167 */     if (true) {
/* 168 */       if (!agg_exprIsNull_0_0) {
/* 169 */         agg_fastAggBuffer_0 = agg_fastHashMap_0.findOrInsert(
/* 170 */           agg_expr_0_0);
/* 171 */       }
/* 172 */     }
/* 173 */     // Cannot find the key in fast hash map, try regular hash map.
/* 174 */     if (agg_fastAggBuffer_0 == null) {
/* 175 */       // generate grouping key
/* 176 */       agg_mutableStateArray_0[0].reset();
/* 177 */
/* 178 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 179 */
/* 180 */       if (agg_exprIsNull_0_0) {
/* 181 */         agg_mutableStateArray_0[0].setNullAt(0);
/* 182 */       } else {
/* 183 */         agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 184 */       }
/* 185 */       int agg_value_4 = 48;
/* 186 */
/* 187 */       if (!agg_exprIsNull_0_0) {
/* 188 */         agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_4);
/* 189 */       }
/* 190 */       if (true) {
/* 191 */         // try to get the buffer from hash map
/* 192 */         agg_unsafeRowAggBuffer_0 =
/* 193 */         agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 194 */       }
/* 195 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 196 */       // aggregation after processing all input rows.
/* 197 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 198 */         if (agg_sorter_0 == null) {
/* 199 */           agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 200 */         } else {
/* 201 */           agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 202 */         }
/* 203 */
/* 204 */         // the hash map had be spilled, it should have enough memory now,
/* 205 */         // try to allocate buffer again.
/* 206 */         agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 207 */           (agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 208 */         if (agg_unsafeRowAggBuffer_0 == null) {
/* 209 */           // failed to allocate the first page
/* 210 */           throw new OutOfMemoryError("No enough memory for aggregation");
/* 211 */         }
/* 212 */       }
/* 213 */
/* 214 */     }
/* 215 */
/* 216 */     if (agg_fastAggBuffer_0 != null) {
/* 217 */       // common sub-expressions
/* 218 */
/* 219 */       // evaluate aggregate function
/* 220 */       long agg_value_10 = agg_fastAggBuffer_0.getLong(0);
/* 221 */
/* 222 */       long agg_value_9 = -1L;
/* 223 */       agg_value_9 = agg_value_10 + 1L;
/* 224 */       // update fast row
/* 225 */       agg_fastAggBuffer_0.setLong(0, agg_value_9);
/* 226 */     } else {
/* 227 */       // common sub-expressions
/* 228 */
/* 229 */       // evaluate aggregate function
/* 230 */       long agg_value_7 = agg_unsafeRowAggBuffer_0.getLong(0);
/* 231 */
/* 232 */       long agg_value_6 = -1L;
/* 233 */       agg_value_6 = agg_value_7 + 1L;
/* 234 */       // update unsafe row buffer
/* 235 */       agg_unsafeRowAggBuffer_0.setLong(0, agg_value_6);
/* 236 */
/* 237 */     }
/* 238 */
/* 239 */   }
/* 240 */
/* 241 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 242 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 243 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 244 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 245 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 246 */       null : (inputadapter_row_0.getUTF8String(0));
/* 247 */
/* 248 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);
/* 249 */       if (shouldStop()) return;
/* 250 */     }
/* 251 */
/* 252 */     agg_fastHashMapIter_0 = agg_fastHashMap_0.rowIterator();
/* 253 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */));
/* 254 */
/* 255 */   }
/* 256 */
/* 257 */   protected void processNext() throws java.io.IOException {
/* 258 */     if (!agg_initAgg_0) {
/* 259 */       agg_initAgg_0 = true;
/* 260 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 261 */       agg_doAggregateWithKeys_0();
/* 262 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 263 */     }
/* 264 */
/* 265 */     // output the result
/* 266 */
/* 267 */     while (agg_fastHashMapIter_0.next()) {
/* 268 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_fastHashMapIter_0.getKey();
/* 269 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_fastHashMapIter_0.getValue();
/* 270 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 271 */
/* 272 */       if (shouldStop()) return;
/* 273 */     }
/* 274 */     agg_fastHashMap_0.close();
/* 275 */
/* 276 */     while (agg_mapIter_0.next()) {
/* 277 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 278 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 279 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 280 */
/* 281 */       if (shouldStop()) return;
/* 282 */     }
/* 283 */
/* 284 */     agg_mapIter_0.close();
/* 285 */     if (agg_sorter_0 == null) {
/* 286 */       agg_hashMap_0.free();
/* 287 */     }
/* 288 */   }
/* 289 */
/* 290 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private agg_FastHashMap_0 agg_fastHashMap_0;
/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter_0;
/* 014 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 017 */   private scala.collection.Iterator inputadapter_input_0;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 019 */
/* 020 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 021 */     this.references = references;
/* 022 */   }
/* 023 */
/* 024 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 025 */     partitionIndex = index;
/* 026 */     this.inputs = inputs;
/* 027 */
/* 028 */     agg_fastHashMap_0 = new agg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());
/* 029 */     agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 032 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 033 */
/* 034 */   }
/* 035 */
/* 036 */   public class agg_FastHashMap_0 {
/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;
/* 038 */     private int[] buckets;
/* 039 */     private int capacity = 1 << 16;
/* 040 */     private double loadFactor = 0.5;
/* 041 */     private int numBuckets = (int) (capacity / loadFactor);
/* 042 */     private int maxSteps = 2;
/* 043 */     private int numRows = 0;
/* 044 */     private Object emptyVBase;
/* 045 */     private long emptyVOff;
/* 046 */     private int emptyVLen;
/* 047 */     private boolean isBatchFull = false;
/* 048 */
/* 049 */     public agg_FastHashMap_0(
/* 050 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,
/* 051 */       InternalRow emptyAggregationBuffer) {
/* 052 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch
/* 053 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);
/* 054 */
/* 055 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));
/* 056 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();
/* 057 */
/* 058 */       emptyVBase = emptyBuffer;
/* 059 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;
/* 060 */       emptyVLen = emptyBuffer.length;
/* 061 */
/* 062 */       buckets = new int[numBuckets];
/* 063 */       java.util.Arrays.fill(buckets, -1);
/* 064 */     }
/* 065 */
/* 066 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key_0) {
/* 067 */       long h = hash(agg_key_0);
/* 068 */       int step = 0;
/* 069 */       int idx = (int) h & (numBuckets - 1);
/* 070 */       while (step < maxSteps) {
/* 071 */         // Return bucket index if it's either an empty slot or already contains the key
/* 072 */         if (buckets[idx] == -1) {
/* 073 */           if (numRows < capacity && !isBatchFull) {
/* 074 */             // creating the unsafe for new entry
/* 075 */             org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter
/* 076 */             = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(
/* 077 */               1, 32);
/* 078 */             agg_rowWriter.reset(); //TODO: investigate if reset or zeroout are actually needed
/* 079 */             agg_rowWriter.zeroOutNullBytes();
/* 080 */             agg_rowWriter.write(0, agg_key_0);
/* 081 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result
/* 082 */             = agg_rowWriter.getRow();
/* 083 */             Object kbase = agg_result.getBaseObject();
/* 084 */             long koff = agg_result.getBaseOffset();
/* 085 */             int klen = agg_result.getSizeInBytes();
/* 086 */
/* 087 */             UnsafeRow vRow
/* 088 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);
/* 089 */             if (vRow == null) {
/* 090 */               isBatchFull = true;
/* 091 */             } else {
/* 092 */               buckets[idx] = numRows++;
/* 093 */             }
/* 094 */             return vRow;
/* 095 */           } else {
/* 096 */             // No more space
/* 097 */             return null;
/* 098 */           }
/* 099 */         } else if (equals(idx, agg_key_0)) {
/* 100 */           return batch.getValueRow(buckets[idx]);
/* 101 */         }
/* 102 */         idx = (idx + 1) & (numBuckets - 1);
/* 103 */         step++;
/* 104 */       }
/* 105 */       // Didn't find it
/* 106 */       return null;
/* 107 */     }
/* 108 */
/* 109 */     private boolean equals(int idx, UTF8String agg_key_0) {
/* 110 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);
/* 111 */       return (row.getUTF8String(0).equals(agg_key_0));
/* 112 */     }
/* 113 */
/* 114 */     private long hash(UTF8String agg_key_0) {
/* 115 */       long agg_hash_0 = 0;
/* 116 */
/* 117 */       int agg_result_0 = 0;
/* 118 */       byte[] agg_bytes_0 = agg_key_0.getBytes();
/* 119 */       for (int i = 0; i < agg_bytes_0.length; i++) {
/* 120 */         int agg_hash_1 = agg_bytes_0[i];
/* 121 */         agg_result_0 = (agg_result_0 ^ (0x9e3779b9)) + agg_hash_1 + (agg_result_0 << 6) + (agg_result_0 >>> 2);
/* 122 */       }
/* 123 */
/* 124 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_0 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);
/* 125 */
/* 126 */       return agg_hash_0;
/* 127 */     }
/* 128 */
/* 129 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {
/* 130 */       return batch.rowIterator();
/* 131 */     }
/* 132 */
/* 133 */     public void close() {
/* 134 */       batch.close();
/* 135 */     }
/* 136 */
/* 137 */   }
/* 138 */
/* 139 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 140 */   throws java.io.IOException {
/* 141 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);
/* 142 */
/* 143 */     boolean agg_isNull_11 = agg_keyTerm_0.isNullAt(0);
/* 144 */     UTF8String agg_value_12 = agg_isNull_11 ?
/* 145 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 146 */     long agg_value_13 = agg_bufferTerm_0.getLong(0);
/* 147 */
/* 148 */     agg_mutableStateArray_0[1].reset();
/* 149 */
/* 150 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 151 */
/* 152 */     if (agg_isNull_11) {
/* 153 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 154 */     } else {
/* 155 */       agg_mutableStateArray_0[1].write(0, agg_value_12);
/* 156 */     }
/* 157 */
/* 158 */     agg_mutableStateArray_0[1].write(1, agg_value_13);
/* 159 */     append((agg_mutableStateArray_0[1].getRow()));
/* 160 */
/* 161 */   }
/* 162 */
/* 163 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {
/* 164 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 165 */     UnsafeRow agg_fastAggBuffer_0 = null;
/* 166 */
/* 167 */     if (true) {
/* 168 */       if (!agg_exprIsNull_0_0) {
/* 169 */         agg_fastAggBuffer_0 = agg_fastHashMap_0.findOrInsert(
/* 170 */           agg_expr_0_0);
/* 171 */       }
/* 172 */     }
/* 173 */     // Cannot find the key in fast hash map, try regular hash map.
/* 174 */     if (agg_fastAggBuffer_0 == null) {
/* 175 */       // generate grouping key
/* 176 */       agg_mutableStateArray_0[0].reset();
/* 177 */
/* 178 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 179 */
/* 180 */       if (agg_exprIsNull_0_0) {
/* 181 */         agg_mutableStateArray_0[0].setNullAt(0);
/* 182 */       } else {
/* 183 */         agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 184 */       }
/* 185 */       int agg_value_4 = 48;
/* 186 */
/* 187 */       if (!agg_exprIsNull_0_0) {
/* 188 */         agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_4);
/* 189 */       }
/* 190 */       if (true) {
/* 191 */         // try to get the buffer from hash map
/* 192 */         agg_unsafeRowAggBuffer_0 =
/* 193 */         agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 194 */       }
/* 195 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 196 */       // aggregation after processing all input rows.
/* 197 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 198 */         if (agg_sorter_0 == null) {
/* 199 */           agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 200 */         } else {
/* 201 */           agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 202 */         }
/* 203 */
/* 204 */         // the hash map had be spilled, it should have enough memory now,
/* 205 */         // try to allocate buffer again.
/* 206 */         agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 207 */           (agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 208 */         if (agg_unsafeRowAggBuffer_0 == null) {
/* 209 */           // failed to allocate the first page
/* 210 */           throw new OutOfMemoryError("No enough memory for aggregation");
/* 211 */         }
/* 212 */       }
/* 213 */
/* 214 */     }
/* 215 */
/* 216 */     if (agg_fastAggBuffer_0 != null) {
/* 217 */       // common sub-expressions
/* 218 */
/* 219 */       // evaluate aggregate function
/* 220 */       long agg_value_10 = agg_fastAggBuffer_0.getLong(0);
/* 221 */
/* 222 */       long agg_value_9 = -1L;
/* 223 */       agg_value_9 = agg_value_10 + 1L;
/* 224 */       // update fast row
/* 225 */       agg_fastAggBuffer_0.setLong(0, agg_value_9);
/* 226 */     } else {
/* 227 */       // common sub-expressions
/* 228 */
/* 229 */       // evaluate aggregate function
/* 230 */       long agg_value_7 = agg_unsafeRowAggBuffer_0.getLong(0);
/* 231 */
/* 232 */       long agg_value_6 = -1L;
/* 233 */       agg_value_6 = agg_value_7 + 1L;
/* 234 */       // update unsafe row buffer
/* 235 */       agg_unsafeRowAggBuffer_0.setLong(0, agg_value_6);
/* 236 */
/* 237 */     }
/* 238 */
/* 239 */   }
/* 240 */
/* 241 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 242 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 243 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 244 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 245 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 246 */       null : (inputadapter_row_0.getUTF8String(0));
/* 247 */
/* 248 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);
/* 249 */       if (shouldStop()) return;
/* 250 */     }
/* 251 */
/* 252 */     agg_fastHashMapIter_0 = agg_fastHashMap_0.rowIterator();
/* 253 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */));
/* 254 */
/* 255 */   }
/* 256 */
/* 257 */   protected void processNext() throws java.io.IOException {
/* 258 */     if (!agg_initAgg_0) {
/* 259 */       agg_initAgg_0 = true;
/* 260 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 261 */       agg_doAggregateWithKeys_0();
/* 262 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 263 */     }
/* 264 */
/* 265 */     // output the result
/* 266 */
/* 267 */     while (agg_fastHashMapIter_0.next()) {
/* 268 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_fastHashMapIter_0.getKey();
/* 269 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_fastHashMapIter_0.getValue();
/* 270 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 271 */
/* 272 */       if (shouldStop()) return;
/* 273 */     }
/* 274 */     agg_fastHashMap_0.close();
/* 275 */
/* 276 */     while (agg_mapIter_0.next()) {
/* 277 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 278 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 279 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 280 */
/* 281 */       if (shouldStop()) return;
/* 282 */     }
/* 283 */
/* 284 */     agg_mapIter_0.close();
/* 285 */     if (agg_sorter_0 == null) {
/* 286 */       agg_hashMap_0.free();
/* 287 */     }
/* 288 */   }
/* 289 */
/* 290 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 40.957454 ms
19/04/11 11:21:58 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared fields: 4
19/04/11 11:21:58 DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
19/04/11 11:21:58 DEBUG ClosureCleaner:      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
19/04/11 11:21:58 DEBUG ClosureCleaner:      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
19/04/11 11:21:58 DEBUG ClosureCleaner:  + inner classes: 1
19/04/11 11:21:58 DEBUG ClosureCleaner:      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:58 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:58 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
19/04/11 11:21:58 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared fields: 4
19/04/11 11:21:58 DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
19/04/11 11:21:58 DEBUG ClosureCleaner:      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
19/04/11 11:21:58 DEBUG ClosureCleaner:      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
19/04/11 11:21:58 DEBUG ClosureCleaner:  + inner classes: 1
19/04/11 11:21:58 DEBUG ClosureCleaner:      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:58 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:58 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
19/04/11 11:21:58 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:58 DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
19/04/11 11:21:58 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:58 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:58 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
19/04/11 11:21:58 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared fields: 1
19/04/11 11:21:58 DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
19/04/11 11:21:58 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:58 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:58 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
19/04/11 11:21:58 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared fields: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
19/04/11 11:21:58 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
19/04/11 11:21:58 DEBUG ClosureCleaner:  + declared methods: 2
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
19/04/11 11:21:58 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
19/04/11 11:21:58 DEBUG ClosureCleaner:  + inner classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer classes: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + outer objects: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
19/04/11 11:21:58 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
19/04/11 11:21:58 DEBUG ClosureCleaner:  + there are no enclosing objects!
19/04/11 11:21:58 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
19/04/11 11:21:58 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
19/04/11 11:21:58 INFO DAGScheduler: Registering RDD 13 (showString at NativeMethodAccessorImpl.java:0)
19/04/11 11:21:58 DEBUG ContextCleaner: Got cleaning task CleanAccum(76)
19/04/11 11:21:58 DEBUG ContextCleaner: Cleaning accumulator 76
19/04/11 11:21:58 INFO ContextCleaner: Cleaned accumulator 76
19/04/11 11:21:58 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
19/04/11 11:21:58 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)
19/04/11 11:21:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
19/04/11 11:21:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
19/04/11 11:21:58 DEBUG DAGScheduler: submitStage(ResultStage 4)
19/04/11 11:21:58 DEBUG DAGScheduler: missing: List(ShuffleMapStage 3)
19/04/11 11:21:58 DEBUG DAGScheduler: submitStage(ShuffleMapStage 3)
19/04/11 11:21:58 DEBUG DAGScheduler: missing: List()
19/04/11 11:21:58 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
19/04/11 11:21:58 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 3)
19/04/11 11:21:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.2 KB, free 366.3 MB)
19/04/11 11:21:58 DEBUG BlockManager: Put block broadcast_3 locally took  1 ms
19/04/11 11:21:58 DEBUG BlockManager: Putting block broadcast_3 without replication took  1 ms
19/04/11 11:21:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.4 KB, free 366.3 MB)
19/04/11 11:21:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.18.111:40565 (size: 16.4 KB, free: 366.3 MB)
19/04/11 11:21:58 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0
19/04/11 11:21:58 DEBUG BlockManager: Told master about block broadcast_3_piece0
19/04/11 11:21:58 DEBUG BlockManager: Put block broadcast_3_piece0 locally took  2 ms
19/04/11 11:21:58 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took  2 ms
19/04/11 11:21:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
19/04/11 11:21:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/04/11 11:21:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/04/11 11:21:58 DEBUG TaskSetManager: Epoch for TaskSet 3.0: 0
19/04/11 11:21:58 DEBUG TaskSetManager: Valid locality levels for TaskSet 3.0: ANY
19/04/11 11:21:58 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 0
19/04/11 11:21:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7771 bytes)
19/04/11 11:21:58 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
19/04/11 11:21:58 DEBUG BlockManager: Getting local block broadcast_3
19/04/11 11:21:58 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
19/04/11 11:21:58 INFO KafkaRDD: Computing topic firewall, partition 0 offsets 0 -> 4000
19/04/11 11:21:58 INFO VerifiableProperties: Verifying properties
19/04/11 11:21:58 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
19/04/11 11:21:58 INFO VerifiableProperties: Property group.id is overridden to mygroup
19/04/11 11:21:58 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
19/04/11 11:21:58 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:58 DEBUG BlockingChannel: Created socket with SO_TIMEOUT = 30000 (requested 30000), SO_RCVBUF = 65536 (requested 65536), SO_SNDBUF = 1313280 (requested -1), connectTimeoutMs = 30000.
19/04/11 11:21:58 DEBUG SimpleConsumer: Disconnecting from docker-kafka:9092
19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 19.221666 ms
19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for 0:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */
/* 032 */     mutableStateArray_0[0].write(0, 0L);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */
/* 032 */     mutableStateArray_0[0].write(0, 0L);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 5.859514 ms
19/04/11 11:21:58 DEBUG TaskMemoryManager: Task 3 acquired 2.0 MB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@64419af
19/04/11 11:21:58 INFO PythonRunner: Times: total = 24, boot = -2136, init = 2149, finish = 11
19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for input[0, bigint, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 7.158462 ms
19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for 0:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */
/* 032 */     mutableStateArray_0[0].write(0, 0L);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 DEBUG TaskMemoryManager: Task 3 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@508a7cbc
19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for input[0, bigint, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:58 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       UTF8String value_2 = isNull_2 ?
/* 039 */       null : (i.getUTF8String(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);
/* 042 */       }
/* 043 */
/* 044 */       int remainder_0 = value_1 % 200;
/* 045 */       if (remainder_0 < 0) {
/* 046 */         value_0=(remainder_0 + 200) % 200;
/* 047 */       } else {
/* 048 */         value_0=remainder_0;
/* 049 */       }
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableStateArray_0[0].setNullAt(0);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(0, value_0);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

19/04/11 11:21:58 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       UTF8String value_2 = isNull_2 ?
/* 039 */       null : (i.getUTF8String(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);
/* 042 */       }
/* 043 */
/* 044 */       int remainder_0 = value_1 % 200;
/* 045 */       if (remainder_0 < 0) {
/* 046 */         value_0=(remainder_0 + 200) % 200;
/* 047 */       } else {
/* 048 */         value_0=remainder_0;
/* 049 */       }
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableStateArray_0[0].setNullAt(0);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(0, value_0);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

19/04/11 11:21:58 INFO CodeGenerator: Code generated in 10.096616 ms
19/04/11 11:21:58 INFO PythonRunner: Times: total = 68, boot = -2127, init = 2144, finish = 51
19/04/11 11:21:58 INFO PythonRunner: Times: total = 107, boot = 5, init = 21, finish = 81
19/04/11 11:21:59 INFO PythonRunner: Times: total = 128, boot = 3, init = 102, finish = 23
19/04/11 11:21:59 DEBUG TaskMemoryManager: Task 3 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@508a7cbc
19/04/11 11:21:59 DEBUG TaskMemoryManager: Task 3 release 2.0 MB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@64419af
19/04/11 11:21:59 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2734 bytes result sent to driver
19/04/11 11:21:59 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 0
19/04/11 11:21:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 223 ms on localhost (executor driver) (1/1)
19/04/11 11:21:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
19/04/11 11:21:59 DEBUG DAGScheduler: ShuffleMapTask finished on driver
19/04/11 11:21:59 INFO DAGScheduler: ShuffleMapStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.238 s
19/04/11 11:21:59 INFO DAGScheduler: looking for newly runnable stages
19/04/11 11:21:59 INFO DAGScheduler: running: Set()
19/04/11 11:21:59 INFO DAGScheduler: waiting: Set(ResultStage 4)
19/04/11 11:21:59 INFO DAGScheduler: failed: Set()
19/04/11 11:21:59 DEBUG MapOutputTrackerMaster: Increasing epoch to 1
19/04/11 11:21:59 DEBUG DAGScheduler: submitStage(ResultStage 4)
19/04/11 11:21:59 DEBUG DAGScheduler: missing: List()
19/04/11 11:21:59 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
19/04/11 11:21:59 DEBUG DAGScheduler: submitMissingTasks(ResultStage 4)
19/04/11 11:21:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.7 KB, free 366.2 MB)
19/04/11 11:21:59 DEBUG BlockManager: Put block broadcast_4 locally took  1 ms
19/04/11 11:21:59 DEBUG BlockManager: Putting block broadcast_4 without replication took  1 ms
19/04/11 11:21:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.0 KB, free 366.2 MB)
19/04/11 11:21:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.18.111:40565 (size: 16.0 KB, free: 366.3 MB)
19/04/11 11:21:59 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0
19/04/11 11:21:59 DEBUG BlockManager: Told master about block broadcast_4_piece0
19/04/11 11:21:59 DEBUG BlockManager: Put block broadcast_4_piece0 locally took  2 ms
19/04/11 11:21:59 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took  2 ms
19/04/11 11:21:59 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
19/04/11 11:21:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
19/04/11 11:21:59 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/04/11 11:21:59 DEBUG TaskSetManager: Epoch for TaskSet 4.0: 1
19/04/11 11:21:59 DEBUG TaskSetManager: Valid locality levels for TaskSet 4.0: ANY
19/04/11 11:21:59 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0
19/04/11 11:21:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7767 bytes)
19/04/11 11:21:59 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
19/04/11 11:21:59 DEBUG BlockManager: Getting local block broadcast_4
19/04/11 11:21:59 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
19/04/11 11:21:59 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 0-1
19/04/11 11:21:59 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
19/04/11 11:21:59 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
19/04/11 11:21:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
19/04/11 11:21:59 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: shuffle_0_0_0
19/04/11 11:21:59 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  8 ms
19/04/11 11:21:59 DEBUG GenerateUnsafeProjection: code for 0:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */
/* 032 */     mutableStateArray_0[0].write(0, 0L);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

19/04/11 11:21:59 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:59 DEBUG TaskMemoryManager: Task 4 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@ccaf1b6
19/04/11 11:21:59 DEBUG GenerateUnsafeProjection: code for input[0, bigint, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

19/04/11 11:21:59 DEBUG TaskMemoryManager: Task 4 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@ccaf1b6
19/04/11 11:21:59 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/04/11 11:21:59 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0
19/04/11 11:21:59 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

19/04/11 11:21:59 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
19/04/11 11:21:59 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
19/04/11 11:21:59 INFO TaskSchedulerImpl: Cancelling stage 4
19/04/11 11:21:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled
19/04/11 11:21:59 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) failed in 0.070 s due to Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
19/04/11 11:21:59 DEBUG DAGScheduler: After removal of stage 4, remaining stages = 1
19/04/11 11:21:59 DEBUG DAGScheduler: After removal of stage 3, remaining stages = 0
19/04/11 11:21:59 INFO DAGScheduler: Job 3 failed: showString at NativeMethodAccessorImpl.java:0, took 0.345072 s
19/04/11 11:21:59 INFO JobScheduler: Finished job streaming job 1554952915000 ms.2 from job set of time 1554952915000 ms
19/04/11 11:21:59 ERROR JobScheduler: Error running job streaming job 1554952915000 ms.2
org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/streaming/util.py", line 68, in call
    r = self.func(t, *rdds)
  File "/home/lance/gemini_task/ps_consumer.py", line 134, in process
    aggdf.show()
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 378, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o240.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more


	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Traceback (most recent call last):
  File "/home/lance/gemini_task/ps_consumer.py", line 143, in <module>
    ssc.awaitTermination()
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/streaming/context.py", line 192, in awaitTermination
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o26.awaitTermination.
: org.apache.spark.SparkException: An exception was raised by Python:
Traceback (most recent call last):
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/streaming/util.py", line 68, in call
    r = self.func(t, *rdds)
  File "/home/lance/gemini_task/ps_consumer.py", line 134, in process
    aggdf.show()
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 378, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/root/.local/share/virtualenvs/gemini_task-p6OkMWYi/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o240.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
	at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:453)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(generated.java:115)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(generated.java:133)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more


	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)
	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

19/04/11 11:21:59 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
19/04/11 11:21:59 DEBUG JobScheduler: Stopping JobScheduler
19/04/11 11:21:59 INFO ReceiverTracker: ReceiverTracker stopped
19/04/11 11:21:59 INFO JobGenerator: Stopping JobGenerator immediately
19/04/11 11:21:59 INFO RecurringTimer: Stopped timer for JobGenerator after time 1554952915000
19/04/11 11:21:59 INFO JobGenerator: Stopped JobGenerator
19/04/11 11:21:59 DEBUG JobScheduler: Stopping job executor
19/04/11 11:21:59 DEBUG JobScheduler: Stopped job executor
19/04/11 11:21:59 INFO JobScheduler: Stopped JobScheduler
19/04/11 11:21:59 INFO StreamingContext: StreamingContext stopped successfully
19/04/11 11:21:59 INFO SparkContext: Invoking stop() from shutdown hook
19/04/11 11:21:59 INFO SparkUI: Stopped Spark web UI at http://192.168.18.111:4040
19/04/11 11:21:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/11 11:21:59 INFO MemoryStore: MemoryStore cleared
19/04/11 11:21:59 INFO BlockManager: BlockManager stopped
19/04/11 11:21:59 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/11 11:21:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/11 11:21:59 INFO SparkContext: Successfully stopped SparkContext
19/04/11 11:21:59 INFO ShutdownHookManager: Shutdown hook called
19/04/11 11:21:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d34fe06-2832-45fe-a2fa-d8346d95630e/pyspark-e5110399-5556-4664-a97f-24c2cb29b48e
19/04/11 11:21:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d45fbcc-5afe-481e-9c79-553be5c6e50d
19/04/11 11:21:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d34fe06-2832-45fe-a2fa-d8346d95630e
